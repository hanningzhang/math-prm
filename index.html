<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Entropy-Regularized Process Reward Model</title>
<!--   <link rel="icon" type="image/x-icon" href="static/images/favicon.ico"> -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Entropy-Regularized Process Reward Model</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://hanningzhang.github.io" target="_blank">Hanning Zhang</a><sup>*1</sup>,</span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=Uag18jcAAAAJ&hl=en" target="_blank">Pengcheng Wang</a><sup>*2</sup>,</span>
                  <span class="author-block">
                    <a href="https://shizhediao.github.io/" target="_blank">Shizhe Diao</a><sup>3</sup>,</span>
                    <span class="author-block">
                    <a href="https://linyongver.github.io/Website/" target="_blank">Yong Lin</a><sup>4</sup>,</span>
                      <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=SBPkYz8AAAAJ&hl=en" target="_blank">Rui Pan</a><sup>1</sup>,</span>
                        <span class="author-block">
                    <a href="https://hendrydong.github.io/" target="_blank">Hanze Dong</a><sup>5</sup>,</span>
                          <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=bqrq-HUAAAAJ&hl=en" target="_blank">Dylan Zhang</a><sup>1</sup>,</span>
                            <span class="author-block">
                    <a href="https://www.pmolchanov.com/" target="_blank">Pavlo Molchanov</a><sup>3</sup>,</span>
                              <span class="author-block">
                    <a href="https://tongzhang-ml.org/" target="_blank">Tong Zhang</a><sup>1</sup></span>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>University of Illinois Urbana-Champaign,</span>
                    <span class="author-block"><sup>2</sup>University of Toronto,</span>
                    <span class="author-block"><sup>3</sup>Nvidia Research,</span>
                    <span class="author-block"><sup>4</sup>Princeton University,</span>
                    <span class="author-block"><sup>5</sup>Salesforce Research</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->

<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Large language models (LLMs) have shown promise in performing complex multi-step reasoning, yet they continue to struggle with mathematical reasoning, often making systematic errors. A promising solution is reinforcement learning (RL) guided by reward models, particularly those focusing on process rewards, which score each intermediate step rather than solely evaluating the final outcome. This approach is more effective at guiding policy models towards correct reasoning trajectories. However, traditional process reward models diverge from typical RL practices and can limit model generalization. In this work, we propose an entropy-regularized process reward model (ER-PRM) that addresses these limitations by integrating KL-regularized Markov Decision Processes (MDP) to balance policy optimization with the need to prevent the policy from shifting too far from its initial distribution.  We derive a novel reward construction method and introduce an automatic labeling mechanism based on the theoretical results. 
Our experiments on the MATH and GSM8K benchmarks demonstrate that ER-PRM consistently outperforms existing process reward models, achieving 1% improvement on GSM8K and 2-3% improvement on MATH under best-of-N evaluation. 
These results highlight the efficacy of entropy-regularization in enhancing LLMs' reasoning capabilities.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

  
<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">Method Overview: Process Reward Labeling</h2>
          <center>
          <img src="static/images/prm-overview.png" alt="Illustration of ER-PRM" class="center-image blend-img-background"/>
          </center>
          <div class="level-set has-text-justified">
            <p>
               llustration of ER-PRM, along with other baseline methods to construct the process reward data and
outcome reward data. The key idea of ER-PRM is to calculate the process reward from the sampling trajectories
under entropy-regularization.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Paper Introduction -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <h2 class="title is-3">Introduction</h2>
        <div class="content has-text-justified">
          <p>
            Complex mathematical reasoning for Large Language Models (LLMs) typically requires multiple steps of reasoning 
            before producing the final answer. 
            This introduces the Process Reward Model (PRM), which assigns a score to each step in the reasoning process, 
            providing more fine-grained feedback.
            It has been shown to outperform Outcome Reward Model (ORM) which only provides an overall score for the entire response.
            To obtain the PRM training data, one prominent method is proposed by Math-Shepherd to automatically label the process reward
            without human annotators.
            The main idea of Math-Shepherd is to interpret the score of each step as its potential to deduce the correct final answer.
            Then, we can sample multiple trajectories starting from the intermediate step, 
            and use the number of correct trajectories as a proxy for this score.
          </p>
          <p>
            Despite the effectiveness, previous automatic labeling strategies consider a traditional Markov Decision Process (MDP), 
            which differs from the typical RL practice used with LLMs.
             Since the introduction of reinforcement learning from human feedback (RLHF),
              the standard approach has been using an entropy-regularized reward relative to a reference model.
            This also applies to RL in reasoning tasks where we formulate the problem as an entropy-regularized MDP.
             In this project, we formulate the multi-step mathematical reasoning task under the entropy-regularized MDP framework,
            derive the mathematical principles of the process reward construction, and propose practical algorithms. 
          </p>
          <p>
          We consider training an LLM for reasoning. 
          Given prompt \(x\), the LLM produces \(L\)-step reasoning chain \(a = [a^1,\dots,a^L]\). 
          The reward \(r(a,x)\) indicates whether the result of the reasoning chain is correct or not. 
          We want to find LLM, denoted by a policy \(\pi_*(a|x)\), that optimizes \(\pi\in \Pi\) with the KL-regularized loss function:
          \[
            \mathcal{L}(\pi) =  -\mathbb{E}_x\mathbb{E}_{a\sim\pi(\cdot|x)}
            \big[r(a,x) - \frac{1}{\eta}\ln{\frac{\pi(a|x)}{\pi_0(a|x)}}\big]
          \]
          where \(\pi_0\) is the initial policy model, a pretrained LLM, and \(\pi\) is the model being fine-tuned.
          </p>
          <p>
            The minimizer for \(\mathcal{L}\) is:
            \[
            \pi_*(a|x) =
            \frac{\pi_0(a|x)e^{\eta r(a,x)}}
            {\mathbb{E}_{a\sim\pi_0}e^{\eta r(a,x)}}
            \propto
            {\pi_0(a|x)e^{\eta r(a,x)}}
            \]
          </p>
          <p>
            Let \(a^{[l]} = [a^1,\dots,a^l]\),
            be partial reasoning up to step \(l\), and \(a^{-[l]} = [a^{l+1},\dots,a^L]\)
            be the completion of the partial reasoning from step \(l+1\). 
            We define the intermediate reward by 
            \(
            e^{\eta r(a^{[l]},x)} = \mathbb{E}_{a^{-[l]}\sim\pi_0}e^{\eta r(a,x)}
            \). Thus, we have:
            \[
              \pi_*(a^{-[l]}|x, a^{[l]}) = \frac{\pi_0(a^{-[l]}|x, a^{[l]})e^{\eta r(a,x)}}{\mathbb{E}_{a^{-[l]}\sim\pi_0(\cdot|x,a^{[l]})}e^{\eta r(a,x)}}
            \]
            And we obtain:
            \[
              \pi_*(a^{[l]}|x) 
              =\frac{\pi_0(a^{[l]}|x)e^{\eta r(a^{[l]},x)}}{\mathbb{E}_{a^{[l]}\sim\pi_0}e^{\eta r(a^{[l]},x)}}
            \]
            \[
              r(a^{[l]}, x) = 
              \frac{1}{\eta} \ln
              \mathbb{E}_{a^{-[l]}\sim\pi_0}e^{\eta r(a,x)}.
              \label{eq:prm}
            \]
            The partial reward according to this formula is our definition of entropy regularized process reward.
            We can optimize partial
          reasoning step \(\pi(a^{[l]}|x)\) using this process reward.
          </p>
          <p>
            An important property of this formulation is that our process reward model can be computed by using the reference policy \(\pi_0\) to generate the completion. 
            In comparison, in traditional RL, the reward depends on the optimal policy that generates the completion. 
            Therefore in the traditional RL, one has to learn reward and policy simultaneously. This is not necessary in our entropy-based approach.
            As one can see, \(\eqref{eq:prm}\) equation employs soft optimism over paths generated by the reference policy. 
            More generally, the reward can be computed using any policy including the optimal policy. In this case, the equivalent formula becomes:
            \[
              r(a^{[l]}, x) = 
              -\frac{1}{\eta}
              \ln{\mathbb{E}}_{a^{-[l]}\sim\pi_*}e^{-\eta r(a,x)} 
              \label{eq:prm-opt}
            \]
            where we have used the following fact to express \(\eqref{eq:prm}\) using \(\pi_*\):
            \[
              \pi_0(a^{-[l]}|x, a^{[l]}) =
               \frac{\pi_*(a^{-[l]}|x, a^{[l]})e^{-\eta r(a,x)}}{\mathbb{E}_{a^{-[l]}\sim\pi_*}e^{-\eta r(a,x)}}
            \]
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Paper Introduction -->

  
<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">Experiment Results</h2>
          <center>
          <img src="static/images/prm-result.png" alt="Results of PRM and ORM" class="center-image blend-img-background"/>
          </center>
          <div class="level-set has-text-justified">
            <p>
               Best-of-N evaluation results on GSM8K and MATH500 datasets with Mistral-MetaMath-7b as the
generator. The Hard-label PRM is the same setting as Math-Shepherd.
            </p>
          </div>
          <center>
          <img src="static/images/rlhf-result.png" alt="Results of RLHF" class="center-image blend-img-background"/>
          </center>
          <div class="level-set has-text-justified">
            <p>
               Zero-shot CoT evaluation for policy model
Mistral-MetaMath-7b improved via Rejection Sampling Fine-tuning (RAFT).
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <h2 class="title is-3">Acknowledgements</h2>
        <div class="content has-text-justified">
          <p>
            We would like to thank Wei Xiong for his support for the organization of the project and his participation in the discussions, which helped improve this work.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
  
<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <div class="content has-text-justified">
          <p>
            If you find our work or code useful, please consider citing our project:
          </p>
        </div>
      <pre><code>@misc{erprm,
  title={Entropy-Regularized Process Reward Model},
  author={Zhang, Hanning and Wang, Pengcheng and Diao, Shizhe and Lin, Yong and Pan, Rui and Dong, Hanze and Zhang, Dylan and Molchanov, Pavlo and Zhang, Tong},
  year={2024},
  publisher = {GitHub},
  journal = {GitHub repository},
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->

  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
